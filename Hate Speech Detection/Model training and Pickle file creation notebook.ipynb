{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import pickle\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language detection model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input file path\n",
    "path = \"C:\\\\Users\\\\Thanis\\\\Desktop\\\\Data Science\\\\Project\\\\multi lang v1\\\\Data\\\\\"\n",
    "#output pickle file path\n",
    "saved_path = 'C:\\\\Users\\\\Thanis\\\\Desktop\\\\Data Science\\\\Project\\\\multi lang v1\\\\Saved model\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinating all the files\n",
    "\n",
    "files= [file for file in os.listdir(path)]\n",
    "all_data= pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    df= pd.read_csv(path+file)\n",
    "    all_data= pd.concat([all_data, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TU    31277\n",
       "ID    13169\n",
       "GR     8743\n",
       "HI     4665\n",
       "FR     4014\n",
       "en     3967\n",
       "AR     3353\n",
       "DA     2960\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating samples\n",
    "tu_train = all_data[all_data.lang == 'TU'].sample(2000)\n",
    "id_train = all_data[all_data.lang == 'ID'].sample(2000)\n",
    "gr_train = all_data[all_data.lang == 'GR'].sample(2000)\n",
    "hi_train = all_data[all_data.lang == 'HI'].sample(2000)\n",
    "fr_train = all_data[all_data.lang == 'FR'].sample(2000)\n",
    "en_train = all_data[all_data.lang == 'en'].sample(2000)\n",
    "ar_train = all_data[all_data.lang == 'AR'].sample(2000)\n",
    "da_train = all_data[all_data.lang == 'DA'].sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenation of the sampled data \n",
    "list_train = [tu_train,id_train,gr_train,hi_train,fr_train,en_train,ar_train,da_train]\n",
    "train = pd.DataFrame()\n",
    "for t in list_train:\n",
    "    train= pd.concat([train, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train.iterrows():\n",
    "    train.loc[index ,\"tweet\"] = re.sub(r'@\\w+', '', train.loc[index ,\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X and Y split\n",
    "train_X = train['tweet']\n",
    "train_y = train['lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lang_transform(train_X, tfidf_vectorizer):\n",
    "    \n",
    "    tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_X)\n",
    "    tfidf_matrix_train = tfidf_matrix_train.todense()\n",
    "    vocabulary = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    return pd.DataFrame(data=tfidf_matrix_train, columns=vocabulary).iloc[:,0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vectors\n",
    "tfidf_lang_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,1),max_features=20000)\n",
    "train_X = train_lang_transform(train_X, tfidf_lang_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model\n",
    "mnb_lang = MultinomialNB(alpha = 0.0005)\n",
    "lang_fitted = mnb_lang.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the tfidf fitted model as pickle file\n",
    "filename = saved_path + 'vectorized_lang_model.sav'\n",
    "pickle.dump(tfidf_lang_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the fitted mode as a pickle file\n",
    "filename = saved_path + 'finalized_lang_model.sav'\n",
    "pickle.dump(lang_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hatespeech detection model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Training Dataset\n",
    "ar = pd.read_csv(path + \"arabic_dataset_modified.csv\")\n",
    "da = pd.read_csv(path + \"danish_dataset_modified.csv\")\n",
    "hi = pd.read_csv(path + \"hindi_dataset_modified.csv\")\n",
    "tu = pd.read_csv(path + \"turk_dataset_modified.csv\")\n",
    "id = pd.read_csv(path + \"indo_dataset_modified.csv\",engine='python')\n",
    "gr = pd.read_csv(path + \"greek_dataset_modified.csv\")\n",
    "fr = pd.read_csv(path + \"french_dataset_modified.csv\")\n",
    "en = pd.read_csv(path + \"english_dataset_modified.csv\")\n",
    "#en = pd.read_csv(path + \"english_dataset_modified_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform(train_X, tfidf_vectorizer):\n",
    "    \n",
    "    tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_X)\n",
    "    tfidf_matrix_train = tfidf_matrix_train.todense()\n",
    "    vocabulary = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    return pd.DataFrame(data=tfidf_matrix_train, columns=vocabulary).iloc[:,0::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_X = en['tweet']\n",
    "train_en_y = en['label']\n",
    "\n",
    "tfidf_en_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,3))\n",
    "train_en_X = train_transform(train_en_X, tfidf_en_vectorizer)\n",
    "\n",
    "#SVM\n",
    "svm_en=LinearSVC(C=5)\n",
    "en_fitted = svm_en.fit(train_en_X, train_en_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'vectorized_en_model.sav'\n",
    "pickle.dump(tfidf_en_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'finalized_en_model.sav'\n",
    "pickle.dump(en_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fr_X = fr['tweet']\n",
    "train_fr_y = fr['label']\n",
    "\n",
    "tfidf_fr_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,3))\n",
    "train_fr_X = train_transform(train_fr_X, tfidf_fr_vectorizer)\n",
    "\n",
    "#SVM\n",
    "svm_fr=LinearSVC(C=5)\n",
    "fr_fitted = svm_fr.fit(train_fr_X, train_fr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'vectorized_fr_model.sav'\n",
    "pickle.dump(tfidf_fr_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'finalized_fr_model.sav'\n",
    "pickle.dump(fr_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ar_X = ar['tweet']\n",
    "train_ar_y = ar['label']\n",
    "\n",
    "tfidf_ar_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,3))\n",
    "train_ar_X = train_transform(train_ar_X, tfidf_ar_vectorizer)\n",
    "\n",
    "#SVM\n",
    "svm_ar=LinearSVC(C=5)\n",
    "ar_fitted = svm_ar.fit(train_ar_X, train_ar_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'vectorized_ar_model.sav'\n",
    "pickle.dump(tfidf_ar_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'finalized_ar_model.sav'\n",
    "pickle.dump(ar_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = id.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in_X = id1['tweet']\n",
    "train_in_y = id1['label']\n",
    "\n",
    "tfidf_in_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,3))\n",
    "train_in_X = train_transform(train_in_X, tfidf_in_vectorizer)\n",
    "\n",
    "#SVM\n",
    "svm_in=LinearSVC(C=5)\n",
    "in_fitted = svm_in.fit(train_in_X, train_in_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'vectorized_in_model.sav'\n",
    "pickle.dump(tfidf_in_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'finalized_in_model.sav'\n",
    "pickle.dump(in_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_da_X = da['tweet']\n",
    "train_da_y = da['label']\n",
    "\n",
    "tfidf_da_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,3))\n",
    "train_da_X = train_transform(train_da_X, tfidf_da_vectorizer)\n",
    "\n",
    "#bagging\n",
    "bag_da=BaggingClassifier(n_estimators=100)\n",
    "da_fitted = bag_da.fit(train_da_X, train_da_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'vectorized_da_model.sav'\n",
    "pickle.dump(tfidf_da_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'finalized_da_model.sav'\n",
    "pickle.dump(da_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hi_X = hi['tweet']\n",
    "train_hi_y = hi['label']\n",
    "\n",
    "tfidf_hi_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,3))\n",
    "train_hi_X = train_transform(train_hi_X, tfidf_hi_vectorizer)\n",
    "\n",
    "#random forest\n",
    "rf_hi=RandomForestClassifier(n_estimators=100)\n",
    "hi_fitted = rf_hi.fit(train_hi_X, train_hi_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'vectorized_hi_model.sav'\n",
    "pickle.dump(tfidf_hi_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'finalized_hi_model.sav'\n",
    "pickle.dump(hi_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu1 = tu.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tu_X = tu1['tweet']\n",
    "train_tu_y = tu1['label']\n",
    "\n",
    "tfidf_tu_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,3))\n",
    "train_tu_X = train_transform(train_tu_X, tfidf_tu_vectorizer)\n",
    "\n",
    "#SVM\n",
    "svm_tu =LinearSVC(C=5)\n",
    "tu_fitted = svm_tu.fit(train_tu_X, train_tu_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'vectorized_tu_model.sav'\n",
    "pickle.dump(tfidf_tu_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'finalized_tu_model.sav'\n",
    "pickle.dump(tu_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1 = gr.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gr_X = gr1['tweet']\n",
    "train_gr_y = gr1['label']\n",
    "\n",
    "tfidf_gr_vectorizer = TfidfVectorizer(analyzer = u'word',ngram_range = (1,3))\n",
    "train_gr_X = train_transform(train_gr_X, tfidf_gr_vectorizer)\n",
    "\n",
    "#SVM\n",
    "svm_gr =LinearSVC(C=5)\n",
    "gr_fitted = svm_gr.fit(train_gr_X, train_gr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'vectorized_gr_model.sav'\n",
    "pickle.dump(tfidf_gr_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = saved_path + 'finalized_gr_model.sav'\n",
    "pickle.dump(gr_fitted, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
